"use server";

import OpenAI from "openai";
import getGitHubImageUrl from "@/lib/getGitHubImageUrl";
import { getOrCreateThreadId } from "@/app/actions/getOrCreateThreadId";
import { db } from "@/db";
import { appConfig } from "@/db/schema";
import { eq } from "drizzle-orm";

type FunctionParams = {
  sessionId: string;
  imageTitle: string;
  imageFilename: string;
  misleadingFeature: string;
  initialIncorrectReasoning: string;
  userCorrection: string;
};

type EvaluationResult = {
  assistantFeedback: string;
  chatbotReasoning: string;
};

export default async function getEvaluationAndUpdatedReasoning(
  params: FunctionParams
): Promise<EvaluationResult> {
  try {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) throw new Error("OpenAI API key not found");

    // ✅ Fetch assistant ID and model from DB
    const [assistantRow, modelRow] = await Promise.all([
      db.query.appConfig.findFirst({
        where: eq(appConfig.configKey, "OPENAI_ASSISTANT_ID"),
      }),
      db.query.appConfig.findFirst({
        where: eq(appConfig.configKey, "OPENAI_MODEL"),
      }),
    ]);

    const assistantId = assistantRow?.configValue;
    const model = modelRow?.configValue || "gpt-4o";

    if (!assistantId) {
      throw new Error("Assistant ID not found in app_config table");
    }

    const openai = new OpenAI({ apiKey });

    const instructions = `
        You are an AI tutor that helps users critically evaluate misleading data visualizations.
        First, you and the user will be given the flawed reasoning about the given misleading data visualization.
        Then, the user will give you the correction about this flawed reasoning.
        
        After receiving the user's correction, you will give a one sentence feedback on the user's correction. 
        Only if when the users' correction is correct or partially capture the misleading feature, you will revise the flawed reasoning according to the user's correction.
        However, if the user's correction is not correct or inappropriate (out of context), then only give a short one sentence encouraging message (the second role).
      `.trim();

    const current = await openai.beta.assistants.retrieve(assistantId);

    if (current.instructions !== instructions || current.model !== model) {
      await openai.beta.assistants.update(assistantId, {
        instructions,
        model,
      });
    }

    const threadId = await getOrCreateThreadId({
      sessionId: params.sessionId,
      imageFilename: params.imageFilename,
    });

    // 1. Give the flawed reasoning and misleading data visualization
    const flawedPrompt = `
      This is the flawed reasonong: ${params.initialIncorrectReasoning} generated by a misled chatbot who is deceived by ${params.misleadingFeature} embedded in this ${params.imageTitle}.
    `;

    await openai.beta.threads.messages.create(threadId, {
      role: "user",
      content: [
        { type: "text", text: flawedPrompt },
        {
          type: "image_url",
          image_url: {
            url: getGitHubImageUrl(params.imageFilename),
          },
        },
      ],
    });

    // 2. 사용자 교정 입력
    await openai.beta.threads.messages.create(threadId, {
      role: "user",
      content: params.userCorrection,
    });

    // 3. 평가 및 reasoning 수정 요청
    const evalPrompt = `
      Evaluate the user's correction: "${params.userCorrection}".
      If they correctly or partially catch the ${params.misleadingFeature} and the impact of ${params.misleadingFeature}, acknowledge it briefly (1 sentence) but do not explicitly give a hint (or mention a misleading feature). -- no direct answer.
      Then, in a new paragraph, based on "${params.userCorrection}", provide a newly revised version of the ${params.initialIncorrectReasoning}.
      This revised version should follow ${params.userCorrection}, and be less deceived by the impact of ${params.misleadingFeature}.
      The revised reasoning should starting by "Revised Flawed Reasoning: ".
      
      However, if the user's correction is wrong, incorrect, vague, or out of context, then give encouraging message (1 sentence) with little thin level of scaffolding for correction, but do not explicitly give hint. -- no direct answer.
      Here, do not provide any revision for the original flawed reasoning.
    `;

    await openai.beta.threads.messages.create(threadId, {
      role: "user",
      content: evalPrompt,
    });

    const run = await openai.beta.threads.runs.create(threadId, {
      assistant_id: assistantId,
    });

    let runStatus;
    do {
      runStatus = await openai.beta.threads.runs.retrieve(threadId, run.id);
      await new Promise((res) => setTimeout(res, 1000));
    } while (runStatus.status !== "completed");

    const messages = await openai.beta.threads.messages.list(threadId);
    const firstContent = messages.data[0].content[0];
    const fullText = "text" in firstContent ? firstContent.text.value : "";

    const [feedback, reasoning] = fullText.split("Revised Flawed Reasoning:");

    return {
      assistantFeedback: feedback?.trim() || "No feedback received.",
      chatbotReasoning: reasoning
        ? "Revised Flawed Reasoning: " + reasoning.trim()
        : "No revised reasoning.",
    };
  } catch (error) {
    console.error("❌ OpenAI API Error:", error);
    throw error;
  }
}
